1:"$Sreact.fragment"
2:I[3719,["17","static/chunks/17-9c04c9413a9f9f1f.js","874","static/chunks/874-6cc630662f3664af.js","862","static/chunks/862-15038af301665bb3.js","177","static/chunks/app/layout-eed31c35182faeaf.js"],"ThemeProvider"]
3:I[768,["17","static/chunks/17-9c04c9413a9f9f1f.js","874","static/chunks/874-6cc630662f3664af.js","862","static/chunks/862-15038af301665bb3.js","177","static/chunks/app/layout-eed31c35182faeaf.js"],"default"]
4:I[7555,[],""]
5:I[1295,[],""]
6:I[2548,["17","static/chunks/17-9c04c9413a9f9f1f.js","874","static/chunks/874-6cc630662f3664af.js","862","static/chunks/862-15038af301665bb3.js","177","static/chunks/app/layout-eed31c35182faeaf.js"],"default"]
7:I[7437,["17","static/chunks/17-9c04c9413a9f9f1f.js","178","static/chunks/178-595a94b9af1e67b5.js","874","static/chunks/874-6cc630662f3664af.js","748","static/chunks/748-fd19dff3bc2de4fe.js","974","static/chunks/app/page-2b35fee41fdba876.js"],"default"]
8:I[9507,["17","static/chunks/17-9c04c9413a9f9f1f.js","178","static/chunks/178-595a94b9af1e67b5.js","874","static/chunks/874-6cc630662f3664af.js","748","static/chunks/748-fd19dff3bc2de4fe.js","974","static/chunks/app/page-2b35fee41fdba876.js"],"default"]
9:I[5218,["17","static/chunks/17-9c04c9413a9f9f1f.js","178","static/chunks/178-595a94b9af1e67b5.js","874","static/chunks/874-6cc630662f3664af.js","748","static/chunks/748-fd19dff3bc2de4fe.js","974","static/chunks/app/page-2b35fee41fdba876.js"],"default"]
10:I[9665,[],"MetadataBoundary"]
12:I[9665,[],"OutletBoundary"]
15:I[4911,[],"AsyncMetadataOutlet"]
17:I[9665,[],"ViewportBoundary"]
19:I[6614,[],""]
:HL["/_next/static/css/281a7a2a9170c50d.css","style"]
a:T5ab,Multiple Instance Learning (MIL) for whole slide image (WSI) analysis in computational pathology often neglects instance-level learning as supervision is typically provided only at the bag level, hindering the integrated consideration of instance and bag-level information during the analysis. In this work, we present LadderMIL, a framework designed to improve MIL through two perspectives: (1) employing instance-level supervision and (2) learning inter-instance contextual information at bag level. Firstly, we propose a novel Coarse-to-Fine Self-Distillation (CFSD) paradigm that probes and distils a network trained with bag-level information to adaptively obtain instance-level labels which could effectively provide the instance-level supervision for the same network in a self-improving way. Secondly, to capture inter-instance contextual information in WSI, we propose a Contextual Encoding Generator (CEG), which encodes the contextual appearance of instances within a bag. We also theoretically and empirically prove the instance-level learnability of CFSD. Our LadderMIL is evaluated on multiple clinically relevant benchmarking tasks including breast cancer receptor status classification, multi-class subtype classification, tumour classification, and prognosis prediction. Average improvements of 8.1%, 11% and 2.4% in AUC, F1-score, and C-index, respectively, are demonstrated across the five benchmarks, compared to the best baseline.b:T7b2,@misc{wu2025laddermil,
  title = {LadderMIL: Multiple Instance Learning with Coarse-to-Fine Self-Distillation},
  booktitle = {Under review},
  author = {Shuyang Wu and Yifu Qiu and Ines P. Nearchou and Sandrine Prost and Jonathan A. Fallowfield and Hideki Ueno and Hitoshi Tsuda and David J. Harrison and Hakan Bilen and Timothy J. Kendall},
  year = {2025},
  pages = {45--52},
  eprint = {2502.02707},
  archivePrefix = {arXiv},
  primaryClass = {cs.CV},
  doi = {https://arxiv.org/abs/2502.02707},
  abstract = {Multiple Instance Learning (MIL) for whole slide image (WSI) analysis in computational pathology often neglects instance-level learning as supervision is typically provided only at the bag level, hindering the integrated consideration of instance and bag-level information during the analysis. In this work, we present LadderMIL, a framework designed to improve MIL through two perspectives: (1) employing instance-level supervision and (2) learning inter-instance contextual information at bag level. Firstly, we propose a novel Coarse-to-Fine Self-Distillation (CFSD) paradigm that probes and distils a network trained with bag-level information to adaptively obtain instance-level labels which could effectively provide the instance-level supervision for the same network in a self-improving way. Secondly, to capture inter-instance contextual information in WSI, we propose a Contextual Encoding Generator (CEG), which encodes the contextual appearance of instances within a bag. We also theoretically and empirically prove the instance-level learnability of CFSD. Our LadderMIL is evaluated on multiple clinically relevant benchmarking tasks including breast cancer receptor status classification, multi-class subtype classification, tumour classification, and prognosis prediction. Average improvements of 8.1%, 11% and 2.4% in AUC, F1-score, and C-index, respectively, are demonstrated across the five benchmarks, compared to the best baseline.}
}c:T9fc,Autologous tooth transplantation requires precise surgical guide design, involving manual tracing of donor tooth contours based on patient cone-beam computed tomography (CBCT) scans. While manual corrections are time-consuming and prone to human errors, deep learning-based approaches show promise in reducing labor and time costs while minimizing errors. However, the application of deep learning techniques in this particular field is yet to be investigated. We aimed to assess the feasibility of replacing the traditional design pipeline with a deep learning-enabled autologous tooth transplantation guide design pipeline. This retrospective cross-sectional study used 79 CBCT images collected at the Guangzhou Medical University Hospital between October 2022 and March 2023. Following preprocessing, a total of 5,070 region of interest images were extracted from 79 CBCT images. Autologous tooth transplantation guide design pipelines, either based on traditional manual design or deep learning-based design. The main outcome variable was the error between the reconstructed model and the gold standard benchmark. We used the third molar extracted clinically as the gold standard and leveraged it as the benchmark for evaluating our reconstructed models from different design pipelines. Both trueness and accuracy were used to evaluate this error. Trueness was assessed using the root mean square (RMS), and accuracy was measured using the standard deviation. The secondary outcome variable was the pipeline efficiency, assessed based on the time cost. Time cost refers to the amount of time required to acquire the third molar model using the pipeline. Data were analyzed using the Kruskal–Wallis test. Statistical significance was set at P < .05. In the surface matching comparison for different reconstructed models, the deep learning group achieved the lowest RMS value (0.335 ± 0.066 mm). There were no significant differences in RMS values between manual design by a senior doctor and deep learning-based design (P = .688), and the standard deviation values did not differ among the 3 groups (P = .103). The deep learning-based design pipeline (0.017 ± 0.001 minutes) provided a faster assessment compared to the manual design pipeline by both senior (19.676 ± 2.386 minutes) and junior doctors (30.613 ± 6.571 minutes) (P < .001). The deep learning-based automatic pipeline exhibited similar performance in surgical guide design for autogenous tooth transplantation compared to manual design by senior doctors, and it minimized time costs.d:Tc04,@article{wei2024autologous,
  title = {Autologous Transplantation Tooth Guide Design Based on Deep Learning},
  author = {Lifen Wei and Shuyang Wu and Zelun Huang and Yaxin Chen and Haoran Zheng and Liping Wang},
  year = {2024},
  month = {mar},
  journal = {Journal of Oral and Maxillofacial Surgery},
  volume = {82},
  number = {3},
  pages = {314-324},
  issn = {0278-2391},
  doi = {10.1016/j.joms.2023.09.014},
  urldate = {2024-12-02},
  langid = {english},
  abstract = {Autologous tooth transplantation requires precise surgical guide design, involving manual tracing of donor tooth contours based on patient cone-beam computed tomography (CBCT) scans. While manual corrections are time-consuming and prone to human errors, deep learning-based approaches show promise in reducing labor and time costs while minimizing errors. However, the application of deep learning techniques in this particular field is yet to be investigated.
  We aimed to assess the feasibility of replacing the traditional design pipeline with a deep learning-enabled autologous tooth transplantation guide design pipeline.
  This retrospective cross-sectional study used 79 CBCT images collected at the Guangzhou Medical University Hospital between October 2022 and March 2023. Following preprocessing, a total of 5,070 region of interest images were extracted from 79 CBCT images.
  Autologous tooth transplantation guide design pipelines, either based on traditional manual design or deep learning-based design.
  The main outcome variable was the error between the reconstructed model and the gold standard benchmark. We used the third molar extracted clinically as the gold standard and leveraged it as the benchmark for evaluating our reconstructed models from different design pipelines. Both trueness and accuracy were used to evaluate this error. Trueness was assessed using the root mean square (RMS), and accuracy was measured using the standard deviation. The secondary outcome variable was the pipeline efficiency, assessed based on the time cost. Time cost refers to the amount of time required to acquire the third molar model using the pipeline.
  Data were analyzed using the Kruskal–Wallis test. Statistical significance was set at P < .05.
  In the surface matching comparison for different reconstructed models, the deep learning group achieved the lowest RMS value (0.335 ± 0.066 mm). There were no significant differences in RMS values between manual design by a senior doctor and deep learning-based design (P = .688), and the standard deviation values did not differ among the 3 groups (P = .103). The deep learning-based design pipeline (0.017 ± 0.001 minutes) provided a faster assessment compared to the manual design pipeline by both senior (19.676 ± 2.386 minutes) and junior doctors (30.613 ± 6.571 minutes) (P < .001).
  The deep learning-based automatic pipeline exhibited similar performance in surgical guide design for autogenous tooth transplantation compared to manual design by senior doctors, and it minimized time costs.
  }
}e:T408,Text-based pedestrian search (TBPS) aims at retrieving target persons from the image gallery through descriptive text queries. Despite remarkable progress in recent state-of-the-art approaches, previous works still struggle to efficiently extract discriminative features from multi-modal data. To address the problem of cross-modal fine-grained text-to-image, we proposed a novel Siamese Contrastive Language-Image Model (SiamCLIM). The model implements textual description and target-person interaction through deep bilateral projection, and siamese network structure to capture the relationship between text and image. Experiments show that our model significantly outperforms the state-of-the-art methods on cross-modal fine-grained matching tasks. We conduct the downstream task experiments on the benchmark dataset CUHK-PEDES and the experimental results demonstrate that our model is state-of-the-art and outperforms the current methods by 11.55%, 11.02%, and 7.76% in terms of top-1, top-5, and top-10 accuracy, respectively.f:T595,@inproceedings{huang2023siamclim,
  title = {SiamCLIM: Text-Based Pedestrian Search Via Multi-Modal Siamese Contrastive Learning},
  booktitle = {IEEE International Conference on Image Processing (ICIP)},
  author = {Huang, Runlin and Wu, Shuyang and Jie, Leiping and Zuo, Xinxin and Zhang, Hui},
  year = {2023},
  pages = {1800-1804},
  doi = {10.1109/ICIP49359.2023.10222660},
  abstract = {Text-based pedestrian search (TBPS) aims at retrieving target persons from the image gallery through descriptive text queries. Despite remarkable progress in recent state-of-the-art approaches, previous works still struggle to efficiently extract discriminative features from multi-modal data. To address the problem of cross-modal fine-grained text-to-image, we proposed a novel Siamese Contrastive Language-Image Model (SiamCLIM). The model implements textual description and target-person interaction through deep bilateral projection, and siamese network structure to capture the relationship between text and image. Experiments show that our model significantly outperforms the state-of-the-art methods on cross-modal fine-grained matching tasks. We conduct the downstream task experiments on the benchmark dataset CUHK-PEDES and the experimental results demonstrate that our model is state-of-the-art and outperforms the current methods by 11.55%, 11.02%, and 7.76% in terms of top-1, top-5, and top-10 accuracy, respectively.}
}0:{"P":null,"b":"q-GK_BZu4kmMJGK8enp9Q","p":"","c":["",""],"i":false,"f":[[["",{"children":["__PAGE__",{}]},"$undefined","$undefined",true],["",["$","$1","c",{"children":[[["$","link","0",{"rel":"stylesheet","href":"/_next/static/css/281a7a2a9170c50d.css","precedence":"next","crossOrigin":"$undefined","nonce":"$undefined"}]],["$","html",null,{"lang":"en","className":"scroll-smooth","suppressHydrationWarning":true,"children":[["$","head",null,{"children":[["$","link",null,{"rel":"icon","href":"/favicon.svg","type":"image/svg+xml"}],["$","link",null,{"rel":"dns-prefetch","href":"https://google-fonts.jialeliu.com"}],["$","link",null,{"rel":"preconnect","href":"https://google-fonts.jialeliu.com","crossOrigin":""}],["$","link",null,{"rel":"preload","as":"style","href":"https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700&family=Crimson+Text:ital,wght@0,400;0,600;1,400&display=swap"}],["$","link",null,{"rel":"stylesheet","id":"gfonts-css","href":"https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700&family=Crimson+Text:ital,wght@0,400;0,600;1,400&display=swap","media":"print"}],["$","script",null,{"dangerouslySetInnerHTML":{"__html":"\n              (function(){\n                var l = document.getElementById('gfonts-css');\n                if (!l) return;\n                if (l.media !== 'all') {\n                  l.addEventListener('load', function(){ try { l.media = 'all'; } catch(e){} });\n                }\n              })();\n            "}}],["$","noscript",null,{"children":["$","link",null,{"rel":"stylesheet","href":"https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700&family=Crimson+Text:ital,wght@0,400;0,600;1,400&display=swap"}]}],["$","script",null,{"dangerouslySetInnerHTML":{"__html":"\n              try {\n                const theme = localStorage.getItem('theme-storage');\n                const parsed = theme ? JSON.parse(theme) : null;\n                const setting = parsed?.state?.theme || 'system';\n                const prefersDark = typeof window !== 'undefined' && window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches;\n                const effective = setting === 'dark' ? 'dark' : (setting === 'light' ? 'light' : (prefersDark ? 'dark' : 'light'));\n                var root = document.documentElement;\n                root.classList.add(effective);\n                root.setAttribute('data-theme', effective);\n              } catch (e) {\n                var root = document.documentElement;\n                root.classList.add('light');\n                root.setAttribute('data-theme', 'light');\n              }\n            "}}]]}],["$","body",null,{"className":"font-sans antialiased","children":["$","$L2",null,{"children":[["$","$L3",null,{"items":[{"title":"About","type":"page","target":"about","href":"/"},{"title":"Publications","type":"page","target":"publications","href":"/publications"},{"title":"Awards","type":"page","target":"awards","href":"/awards"},{"title":"CV","type":"page","target":"cv","href":"/cv"}],"siteTitle":"Shuyang Wu","enableOnePageMode":false}],["$","main",null,{"className":"min-h-screen pt-16 lg:pt-20","children":["$","$L4",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L5",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":[[["$","title",null,{"children":"404: This page could not be found."}],["$","div",null,{"style":{"fontFamily":"system-ui,\"Segoe UI\",Roboto,Helvetica,Arial,sans-serif,\"Apple Color Emoji\",\"Segoe UI Emoji\"","height":"100vh","textAlign":"center","display":"flex","flexDirection":"column","alignItems":"center","justifyContent":"center"},"children":["$","div",null,{"children":[["$","style",null,{"dangerouslySetInnerHTML":{"__html":"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}"}}],["$","h1",null,{"className":"next-error-h1","style":{"display":"inline-block","margin":"0 20px 0 0","padding":"0 23px 0 0","fontSize":24,"fontWeight":500,"verticalAlign":"top","lineHeight":"49px"},"children":404}],["$","div",null,{"style":{"display":"inline-block"},"children":["$","h2",null,{"style":{"fontSize":14,"fontWeight":400,"lineHeight":"49px","margin":0},"children":"This page could not be found."}]}]]}]}]],[]],"forbidden":"$undefined","unauthorized":"$undefined"}]}],["$","$L6",null,{"lastUpdated":"December 4, 2025"}]]}]}]]}]]}],{"children":["__PAGE__",["$","$1","c",{"children":[["$","div",null,{"className":"max-w-6xl mx-auto px-4 sm:px-6 lg:px-8 py-8 bg-background min-h-screen","children":["$","div",null,{"className":"grid grid-cols-1 lg:grid-cols-3 gap-12","children":[["$","div",null,{"className":"lg:col-span-1","children":["$","$L7",null,{"author":{"name":"Shuyang (Frank) Wu","title":"PhD Candidate","institution":"University of Edinburgh","avatar":"/bio.jpg"},"social":{"email":"frank.wu@ed.ac.uk","location":"School for Regeneration and Repair","location_url":"https://www.google.com/maps/place/Institute+for+Regeneration+and+Repair+(South+Building)/@55.9205993,-3.1331385,17z/data=!3m1!4b1!4m6!3m5!1s0x4887b9168c94d9e7:0x2edc4541bc4a80ba!8m2!3d55.9205993!4d-3.1305636!16s%2Fg%2F11sv1mhx8s?entry=ttu&g_ep=EgoyMDI1MTIwMi4wIKXMDSoASAFQAw%3D%3D","location_details":["4-5 Little France Drive, Edinburgh BioQuarter,","Edinburgh EH16 4UU"],"google_scholar":"https://scholar.google.com/citations?user=SuKTqhcAAAAJ&hl=en","orcid":"","github":"https://github.com/franksyng","linkedin":"linkedin.com/in/frank-shuyang-wu"},"features":{"enable_likes":true,"enable_one_page_mode":false},"researchInterests":["Machine Learning","Computational Pathology","Medical Image Analysis"]}]}],["$","div",null,{"className":"lg:col-span-2 space-y-8","children":[["$","section","about",{"id":"about","className":"scroll-mt-24 space-y-8","children":[[["$","$L8","about",{"content":"I am a PhD candidate at the School of Regeneration and Repair, University of Edinburgh, advised by [Prof Timothy Kendall](https://edwebprofiles.ed.ac.uk/profile/timothy-kendall) (Medical), [Dr Sandrine Prost](https://pathology.ed.ac.uk/people/staff-and-students/dr-sandrine-prost) (Medical) and [Dr Hakan Bilen](https://homepages.inf.ed.ac.uk/hbilen/) (Informatics).\n\nPrior to this, I obtained my MSc degree in Cognitive Science at the University of Edinburgh and my BSc degree with honour in Computer Science and Technology at the Beijing Normal - Hong Kong Baptist University. Both of my bachelor's and master's degree worked on medical image analysis, specifically MRI and histopathology images.\n\nMy current research focuses on machine learning, computational pathology and medical image analysis.\n","title":"About"}],["$","$L9","featured_publications",{"publications":[{"id":"wu2025laddermil","title":"LadderMIL: Multiple Instance Learning with Coarse-to-Fine Self-Distillation","authors":[{"name":"Shuyang Wu","isHighlighted":false,"isCorresponding":true,"isCoAuthor":true},{"name":"Yifu Qiu","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Ines P. Nearchou","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Sandrine Prost","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Jonathan A. Fallowfield","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Hideki Ueno","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Hitoshi Tsuda","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"David J. Harrison","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Hakan Bilen","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Timothy J. Kendall","isHighlighted":false,"isCorresponding":true,"isCoAuthor":false}],"year":2025,"type":"preprint","status":"published","tags":[],"keywords":"$0:f:0:1:2:children:1:props:children:0:props:children:props:children:1:props:children:0:props:children:0:1:props:publications:0:tags","researchArea":"machine-learning","journal":"","conference":"Under review","pages":"45--52","doi":"https://arxiv.org/abs/2502.02707","code":"https://github.com/franksyng/LadderMIL","abstract":"$a","description":"","selected":true,"bibtex":"$b"},{"id":"wei2024autologous","title":"Autologous Transplantation Tooth Guide Design Based on Deep Learning","authors":[{"name":"Lifen Wei","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Shuyang Wu","isHighlighted":false,"isCorresponding":false,"isCoAuthor":true},{"name":"Zelun Huang","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Yaxin Chen","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Haoran Zheng","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Liping Wang","isHighlighted":false,"isCorresponding":true,"isCoAuthor":false}],"year":2024,"month":"3","type":"journal","status":"published","tags":[],"keywords":"$0:f:0:1:2:children:1:props:children:0:props:children:props:children:1:props:children:0:props:children:0:1:props:publications:1:tags","researchArea":"machine-learning","journal":"Journal of Oral and Maxillofacial Surgery","conference":"","volume":"82","issue":"3","pages":"314-324","doi":"10.1016/j.joms.2023.09.014","abstract":"$c","description":"","selected":true,"bibtex":"$d"},{"id":"huang2023siamclim","title":"SiamCLIM: Text-Based Pedestrian Search Via Multi-Modal Siamese Contrastive Learning","authors":[{"name":"Runlin Huang","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Shuyang Wu","isHighlighted":false,"isCorresponding":false,"isCoAuthor":true},{"name":"Leiping Jie","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Xinxin Zuo","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Hui Zhang","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false}],"year":2023,"type":"conference","status":"published","tags":[],"keywords":"$0:f:0:1:2:children:1:props:children:0:props:children:props:children:1:props:children:0:props:children:0:1:props:publications:2:tags","researchArea":"machine-learning","journal":"","conference":"IEEE International Conference on Image Processing (ICIP)","pages":"1800-1804","doi":"10.1109/ICIP49359.2023.10222660","abstract":"$e","description":"","selected":true,"bibtex":"$f"}],"title":"Selected Publications","enableOnePageMode":false}]],false,false,false]}]]}]]}]}],["$","$L10",null,{"children":"$L11"}],null,["$","$L12",null,{"children":["$L13","$L14",["$","$L15",null,{"promise":"$@16"}]]}]]}],{},null,false]},null,false],["$","$1","h",{"children":[null,["$","$1","LkfXzXUvPCjdT61x0-hWO",{"children":[["$","$L17",null,{"children":"$L18"}],null]}],null]}],false]],"m":"$undefined","G":["$19","$undefined"],"s":false,"S":true}
1a:"$Sreact.suspense"
1b:I[4911,[],"AsyncMetadata"]
11:["$","$1a",null,{"fallback":null,"children":["$","$L1b",null,{"promise":"$@1c"}]}]
14:null
18:[["$","meta","0",{"charSet":"utf-8"}],["$","meta","1",{"name":"viewport","content":"width=device-width, initial-scale=1"}]]
13:null
1c:{"metadata":[["$","title","0",{"children":"Shuyang Wu"}],["$","meta","1",{"name":"description","content":"PhD candidate at the University of Edinburgh."}],["$","meta","2",{"name":"author","content":"Shuyang (Frank) Wu"}],["$","meta","3",{"name":"keywords","content":"Shuyang (Frank) Wu,PhD,Research,University of Edinburgh"}],["$","meta","4",{"name":"creator","content":"Shuyang (Frank) Wu"}],["$","meta","5",{"name":"publisher","content":"Shuyang (Frank) Wu"}],["$","meta","6",{"property":"og:title","content":"Shuyang Wu"}],["$","meta","7",{"property":"og:description","content":"PhD candidate at the University of Edinburgh."}],["$","meta","8",{"property":"og:site_name","content":"Shuyang (Frank) Wu's Academic Website"}],["$","meta","9",{"property":"og:locale","content":"en_US"}],["$","meta","10",{"property":"og:type","content":"website"}],["$","meta","11",{"name":"twitter:card","content":"summary"}],["$","meta","12",{"name":"twitter:title","content":"Shuyang Wu"}],["$","meta","13",{"name":"twitter:description","content":"PhD candidate at the University of Edinburgh."}],["$","link","14",{"rel":"icon","href":"/favicon.svg"}]],"error":null,"digest":"$undefined"}
16:{"metadata":"$1c:metadata","error":null,"digest":"$undefined"}
